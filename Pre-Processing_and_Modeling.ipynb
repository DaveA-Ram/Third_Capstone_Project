{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8370f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run Data_Wrangling_and_EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d30a5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing up the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1c74024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the script for the embedding layer as part of the pre-processing step. The embedding layer will convert the textual\n",
    "#data into numeric data and serves as the first layer for Keras' deep learning models.\n",
    "\n",
    "#Implementing tokenizer class to create a word-to-index dictionary. Key:value --> word:unique index\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67b30107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting max size of each list to 100; longer lists will be truncated and shorter lists will be 'padded' with 0's at the end\n",
    "#of those lists.\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f011350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove embeddings used to create feature matrix. Loading glove embeddings to create a dictionary where keys:values are\n",
    "#words:embeddings lists\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embedded_dict = dict()\n",
    "g_file = open('glove.6B.100d.txt', encoding='utf8')\n",
    "\n",
    "for line in g_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embedded_dict [word] = vector_dimensions\n",
    "g_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0876531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the embedding matrix. Number of rows will match the number of words and each columns (100 in total) will have the\n",
    "#Glove word embeddings.\n",
    "embed_mat = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embed_vec = embedded_dict.get(word)\n",
    "    if embed_vec is not None:\n",
    "        embed_mat[index] = embed_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83467367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1: text classification with SNN (simple neural network)\n",
    "snn = Sequential()\n",
    "embedded_layer = Embedding(vocab_size, 100, weights=[embed_mat], input_length=maxlen, trainable=False)\n",
    "snn.add(embedded_layer)\n",
    "\n",
    "snn.add(Flatten())\n",
    "snn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04e9251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 100)          1899000   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 10000)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 10001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,909,001\n",
      "Trainable params: 10,001\n",
      "Non-trainable params: 1,899,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Compiling the SNN model and getting a summary of the parameters\n",
    "snn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(snn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca239b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and instantiating a callback (early stopping) to control for overfitting\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "call_back = EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e12c76a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "39/39 [==============================] - 1s 18ms/step - loss: 0.1752 - acc: 0.9417 - val_loss: 0.8895 - val_acc: 0.7209\n",
      "Epoch 2/6\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.1754 - acc: 0.9415 - val_loss: 0.8883 - val_acc: 0.7209\n",
      "Epoch 3/6\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 0.1750 - acc: 0.9407 - val_loss: 0.8897 - val_acc: 0.7176\n",
      "Epoch 4/6\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1752 - acc: 0.9411 - val_loss: 0.8909 - val_acc: 0.7192\n"
     ]
    }
   ],
   "source": [
    "#Training the model on the train set\n",
    "history = snn.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2, callbacks=call_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "662d1615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 2ms/step - loss: 0.8134 - acc: 0.7420\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the performance of the model\n",
    "result = snn.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b72ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score: 0.8134480118751526\n",
      "test accuracy: 0.7419566512107849\n"
     ]
    }
   ],
   "source": [
    "#Verifying both the accuracy and loss of the testing set\n",
    "print('test score:', result[0])\n",
    "print('test accuracy:', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0aeded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: text classification with CNN (Convolutional Neural Network)\n",
    "from keras.layers import Conv1D\n",
    "\n",
    "cnn = Sequential()\n",
    "\n",
    "embed_layer = Embedding(vocab_size, 100, weights=[embed_mat], input_length=maxlen, trainable=False)\n",
    "cnn.add(embed_layer)\n",
    "\n",
    "cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn.add(GlobalMaxPooling1D())\n",
    "cnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc31c620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 100)          1899000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 128)           64128     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,963,257\n",
      "Trainable params: 64,257\n",
      "Non-trainable params: 1,899,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Compiling the CNN model and getting a summary of the parameters\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "705c05f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "39/39 [==============================] - 3s 52ms/step - loss: 0.5069 - acc: 0.7541 - val_loss: 0.4929 - val_acc: 0.7619\n",
      "Epoch 2/6\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.4044 - acc: 0.8210 - val_loss: 0.4577 - val_acc: 0.7898\n",
      "Epoch 3/6\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.3614 - acc: 0.8454 - val_loss: 0.4600 - val_acc: 0.7931\n",
      "Epoch 4/6\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.3350 - acc: 0.8580 - val_loss: 0.4563 - val_acc: 0.7980\n",
      "Epoch 5/6\n",
      "39/39 [==============================] - 2s 46ms/step - loss: 0.2992 - acc: 0.8789 - val_loss: 0.5246 - val_acc: 0.7701\n",
      "Epoch 6/6\n",
      "39/39 [==============================] - 2s 47ms/step - loss: 0.2855 - acc: 0.8896 - val_loss: 0.4580 - val_acc: 0.7947\n"
     ]
    }
   ],
   "source": [
    "#Training the model on the training set\n",
    "history_2 = cnn.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d622a965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 5ms/step - loss: 0.4514 - acc: 0.7978\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the performance of the model\n",
    "result_2 = cnn.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c0e26e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score: 0.4513889253139496\n",
      "test accuracy 0.7977675795555115\n"
     ]
    }
   ],
   "source": [
    "#Verifying both the accuracy and loss of the testing set\n",
    "print(\"test score:\", result_2[0])\n",
    "print(\"test accuracy\", result_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d4a1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3: text classification with RNN (Recurrent Neural Netowrk)\n",
    "from keras.layers import LSTM\n",
    "\n",
    "rnn = Sequential()\n",
    "embedd_layer = Embedding(vocab_size, 100, weights=[embed_mat], input_length=maxlen, trainable=False)\n",
    "rnn.add(embedd_layer)\n",
    "rnn.add(LSTM(128))\n",
    "\n",
    "rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d89c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 100, 100)          1899000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,016,377\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 1,899,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Compiling the RNN model and getting a summary of the parameters\n",
    "rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66aed3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "39/39 [==============================] - 9s 175ms/step - loss: 0.6841 - acc: 0.5696 - val_loss: 0.6885 - val_acc: 0.5616\n",
      "Epoch 2/6\n",
      "39/39 [==============================] - 6s 163ms/step - loss: 0.6846 - acc: 0.5714 - val_loss: 0.6868 - val_acc: 0.5616\n",
      "Epoch 3/6\n",
      "39/39 [==============================] - 6s 148ms/step - loss: 0.6840 - acc: 0.5714 - val_loss: 0.6856 - val_acc: 0.5616\n",
      "Epoch 4/6\n",
      "39/39 [==============================] - 6s 148ms/step - loss: 0.6837 - acc: 0.5714 - val_loss: 0.6858 - val_acc: 0.5616\n",
      "Epoch 5/6\n",
      "39/39 [==============================] - 7s 167ms/step - loss: 0.6831 - acc: 0.5714 - val_loss: 0.6856 - val_acc: 0.5616\n",
      "Epoch 6/6\n",
      "39/39 [==============================] - 7s 171ms/step - loss: 0.6830 - acc: 0.5714 - val_loss: 0.6855 - val_acc: 0.5616\n"
     ]
    }
   ],
   "source": [
    "#Training the model on the training set\n",
    "history_3 = rnn.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ee8e153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6826 - acc: 0.5739\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the performance of the model\n",
    "result_3 = rnn.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4098ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score: 0.6825509071350098\n",
      "test accuracy: 0.5738673806190491\n"
     ]
    }
   ],
   "source": [
    "#Verifying both the accuracy and loss of the testing set\n",
    "print('test score:', result_3[0])\n",
    "print('test accuracy:', result_3[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
